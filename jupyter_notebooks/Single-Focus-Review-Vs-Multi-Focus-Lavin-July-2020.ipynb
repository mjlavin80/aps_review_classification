{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "- import various helpers, load data, select reviews by status and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *\n",
    "import database.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full text from db\n",
    "aps_rows = models.Review().query.filter(models.Review.status.in_(('needs_crosscheck', 'needs_details', 'done'))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('/Volumes/TOSHIBA EXT/datasets/nyt_reviews_datastore.db')\n",
    "c = conn.cursor()\n",
    "# query nyt reviews and not\n",
    "nyt_rows = c.execute(\"SELECT * FROM metadata WHERE review_type IN ('not_review', 'multi', 'cluster', 'really_multi', 'single_focus')\").fetchall()\n",
    "len(nyt_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import various from scikit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-work vs. Multi-work Reviews\n",
    "\n",
    "- The exemplar of a single-work review is very clear, as is the exemplar of review that covers more than one \n",
    "- Complications and edge cases arise when it's predominantly a review of one book, with a section that compares it to another book, or in that there is a great variety to multi-work reviews. Some columns like \"Latest Fiction\" are scanned as separate single work reviews, some as one object. In general, I have found it desirable to isolate clear single-work reviews from others for information extraction or review classification tasks, but other methods wouldn't require this.\n",
    "- It may be desirable to target multi-work reviews if, for example, you want \"in the same review\" to be edge weights in a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_single = [i for i in aps_rows if i.review_type == 'single_focus']\n",
    "aps_not_single = [i for i in aps_rows if i.review_type in ('multi', 'cluster')]\n",
    "#len(aps_single), len(aps_not_single) >>> (1003, 550)\n",
    "aps_list_of_full_txt = [i.full_text for i in aps_single] + [i.full_text for i in aps_not_single]\n",
    "# make \"true labels\" (0s and 1s so scikit learn can score them)\n",
    "aps_labels = [0 for i in range(len(aps_single))] + [1 for i in range(len(aps_not_single))]\n",
    "#len(aps_list_of_full_txt) == len(aps_labels) >>> True\n",
    "\n",
    "# set up logistic regression with labels\n",
    "v = CountVectorizer()\n",
    "X = v.fit_transform(aps_list_of_full_txt)\n",
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "\n",
    "# split the rows into training data, training labels, test data, and test labels\n",
    "# test on 33% of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, aps_labels, test_size=0.33, random_state=81)\n",
    "\n",
    "# instantiate the model and fit to the training data\n",
    "lr = LogisticRegression(class_weight={0:0.35, 1:0.65})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lr, Z, aps_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single-work review': {'f1': 0.8204334365325077,\n",
       "  'precision': 0.8412698412698413,\n",
       "  'recall': 0.8006042296072508},\n",
       " 'multi-work review': {'f1': 0.6947368421052631,\n",
       "  'precision': 0.6666666666666666,\n",
       "  'recall': 0.7252747252747253},\n",
       " 'accuracy': 0.7738791423001949}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make label predictions\n",
    "results = lr.predict(X_test)\n",
    "\n",
    "# generate probabilities for each label\n",
    "probs = lr.predict_proba(X_test)\n",
    "\n",
    "scores = {}\n",
    "# generate f1, precision, recall, and accuracy scores\n",
    "# I will discuss each of these in the lesson\n",
    "for y,z in [(\"single-work review\",0),(\"multi-work review\",1)]:\n",
    "    scores[y] = {}\n",
    "    scores[y][\"f1\"] = f1_score(y_test, results, pos_label=z, average='binary')  \n",
    "    scores[y][\"precision\"] = precision_score(y_test, results, pos_label=z, average='binary')\n",
    "    scores[y][\"recall\"] = recall_score(y_test, results, pos_label=z, average='binary')\n",
    "\n",
    "scores[\"accuracy\"] = accuracy_score(y_test, results)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>he</td>\n",
       "      <td>-0.721452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>his</td>\n",
       "      <td>-0.668302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>that</td>\n",
       "      <td>-0.667718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>-0.639716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>him</td>\n",
       "      <td>-0.435286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>not</td>\n",
       "      <td>-0.409498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>had</td>\n",
       "      <td>-0.405744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>were</td>\n",
       "      <td>-0.373912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>to</td>\n",
       "      <td>-0.308820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>as</td>\n",
       "      <td>-0.307325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>be</td>\n",
       "      <td>-0.300616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>would</td>\n",
       "      <td>-0.272713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>us</td>\n",
       "      <td>-0.264312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>you</td>\n",
       "      <td>-0.255627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>my</td>\n",
       "      <td>-0.248861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>did</td>\n",
       "      <td>-0.237361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>if</td>\n",
       "      <td>-0.227858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>when</td>\n",
       "      <td>-0.226263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>never</td>\n",
       "      <td>-0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>only</td>\n",
       "      <td>-0.221687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>can</td>\n",
       "      <td>-0.218413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>or</td>\n",
       "      <td>-0.209664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>lie</td>\n",
       "      <td>-0.203932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>all</td>\n",
       "      <td>-0.199105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>chapter</td>\n",
       "      <td>-0.198836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>where</td>\n",
       "      <td>-0.196822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>at</td>\n",
       "      <td>-0.193704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>upon</td>\n",
       "      <td>-0.185438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>great</td>\n",
       "      <td>-0.184933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>schouler</td>\n",
       "      <td>-0.184918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term      coef\n",
       "0         he -0.721452\n",
       "1        his -0.668302\n",
       "2       that -0.667718\n",
       "3        was -0.639716\n",
       "4        him -0.435286\n",
       "5        not -0.409498\n",
       "6        had -0.405744\n",
       "7       were -0.373912\n",
       "8         to -0.308820\n",
       "9         as -0.307325\n",
       "10        be -0.300616\n",
       "11     would -0.272713\n",
       "12        us -0.264312\n",
       "13       you -0.255627\n",
       "14        my -0.248861\n",
       "15       did -0.237361\n",
       "16        if -0.227858\n",
       "17      when -0.226263\n",
       "18     never -0.226000\n",
       "19      only -0.221687\n",
       "20       can -0.218413\n",
       "21        or -0.209664\n",
       "22       lie -0.203932\n",
       "23       all -0.199105\n",
       "24   chapter -0.198836\n",
       "25     where -0.196822\n",
       "26        at -0.193704\n",
       "27      upon -0.185438\n",
       "28     great -0.184933\n",
       "29  schouler -0.184918"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = []\n",
    "coefs = []\n",
    "for key,val in v.vocabulary_.items():\n",
    "    terms.append(key)\n",
    "    coefs.append(lr.coef_[0][val])\n",
    "\n",
    "# this block produces a dataframe with the top 30 terms associated with label 0\n",
    "df_coef = pd.DataFrame()\n",
    "df_coef['term'] = terms\n",
    "df_coef['coef'] = coefs\n",
    "df_coef = df_coef.sort_values(by='coef').reset_index(drop=True)\n",
    "df_coef.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>69454</td>\n",
       "      <td>illustrated</td>\n",
       "      <td>0.441490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69455</td>\n",
       "      <td>edited</td>\n",
       "      <td>0.443003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69456</td>\n",
       "      <td>contains</td>\n",
       "      <td>0.451154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69457</td>\n",
       "      <td>edition</td>\n",
       "      <td>0.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69458</td>\n",
       "      <td>stories</td>\n",
       "      <td>0.482132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69459</td>\n",
       "      <td>boston</td>\n",
       "      <td>0.485738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69460</td>\n",
       "      <td>series</td>\n",
       "      <td>0.494175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69461</td>\n",
       "      <td>00</td>\n",
       "      <td>0.498977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69462</td>\n",
       "      <td>books</td>\n",
       "      <td>0.550457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69463</td>\n",
       "      <td>mr</td>\n",
       "      <td>0.553731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69464</td>\n",
       "      <td>25</td>\n",
       "      <td>0.562756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69465</td>\n",
       "      <td>volume</td>\n",
       "      <td>0.564269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69466</td>\n",
       "      <td>poems</td>\n",
       "      <td>0.567882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69467</td>\n",
       "      <td>are</td>\n",
       "      <td>0.582949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69468</td>\n",
       "      <td>illustrations</td>\n",
       "      <td>0.637125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69469</td>\n",
       "      <td>pp</td>\n",
       "      <td>0.650341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69470</td>\n",
       "      <td>50</td>\n",
       "      <td>0.660585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69471</td>\n",
       "      <td>price</td>\n",
       "      <td>0.665877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69472</td>\n",
       "      <td>cents</td>\n",
       "      <td>0.701436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69473</td>\n",
       "      <td>story</td>\n",
       "      <td>0.767137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69474</td>\n",
       "      <td>in</td>\n",
       "      <td>0.776416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69475</td>\n",
       "      <td>york</td>\n",
       "      <td>0.829119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69476</td>\n",
       "      <td>book</td>\n",
       "      <td>0.970012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69477</td>\n",
       "      <td>new</td>\n",
       "      <td>1.031195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69478</td>\n",
       "      <td>is</td>\n",
       "      <td>1.159626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69479</td>\n",
       "      <td>the</td>\n",
       "      <td>1.186988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69480</td>\n",
       "      <td>of</td>\n",
       "      <td>1.401769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69481</td>\n",
       "      <td>co</td>\n",
       "      <td>1.469271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69482</td>\n",
       "      <td>by</td>\n",
       "      <td>1.782722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69483</td>\n",
       "      <td>and</td>\n",
       "      <td>1.797663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                term      coef\n",
       "69454    illustrated  0.441490\n",
       "69455         edited  0.443003\n",
       "69456       contains  0.451154\n",
       "69457        edition  0.468600\n",
       "69458        stories  0.482132\n",
       "69459         boston  0.485738\n",
       "69460         series  0.494175\n",
       "69461             00  0.498977\n",
       "69462          books  0.550457\n",
       "69463             mr  0.553731\n",
       "69464             25  0.562756\n",
       "69465         volume  0.564269\n",
       "69466          poems  0.567882\n",
       "69467            are  0.582949\n",
       "69468  illustrations  0.637125\n",
       "69469             pp  0.650341\n",
       "69470             50  0.660585\n",
       "69471          price  0.665877\n",
       "69472          cents  0.701436\n",
       "69473          story  0.767137\n",
       "69474             in  0.776416\n",
       "69475           york  0.829119\n",
       "69476           book  0.970012\n",
       "69477            new  1.031195\n",
       "69478             is  1.159626\n",
       "69479            the  1.186988\n",
       "69480             of  1.401769\n",
       "69481             co  1.469271\n",
       "69482             by  1.782722\n",
       "69483            and  1.797663"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coef.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, Z, aps_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.32 (+/- 1.26)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lasso = linear_model.Lasso()\n",
    "scores = cross_val_score(lasso, Z, aps_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(neigh, Z, aps_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='sigmoid')\n",
    "scores = cross_val_score(svm, Z, aps_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "v = CountVectorizer(max_features=10000)\n",
    "X = v.fit_transform(aps_list_of_full_txt)\n",
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(10000,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, aps_labels, test_size=0.33, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1040/1040 [==============================] - 0s 137us/step - loss: 0.3534 - acc: 0.9346\n",
      "Epoch 2/30\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.3329 - acc: 0.9394\n",
      "Epoch 3/30\n",
      "1040/1040 [==============================] - 0s 106us/step - loss: 0.3126 - acc: 0.9481\n",
      "Epoch 4/30\n",
      "1040/1040 [==============================] - 0s 96us/step - loss: 0.2926 - acc: 0.9587\n",
      "Epoch 5/30\n",
      "1040/1040 [==============================] - 0s 106us/step - loss: 0.2732 - acc: 0.9654\n",
      "Epoch 6/30\n",
      "1040/1040 [==============================] - 0s 100us/step - loss: 0.2538 - acc: 0.9692\n",
      "Epoch 7/30\n",
      "1040/1040 [==============================] - 0s 102us/step - loss: 0.2352 - acc: 0.9731\n",
      "Epoch 8/30\n",
      "1040/1040 [==============================] - 0s 98us/step - loss: 0.2173 - acc: 0.9865\n",
      "Epoch 9/30\n",
      "1040/1040 [==============================] - 0s 113us/step - loss: 0.2003 - acc: 0.9894\n",
      "Epoch 10/30\n",
      "1040/1040 [==============================] - 0s 108us/step - loss: 0.1840 - acc: 0.9923\n",
      "Epoch 11/30\n",
      "1040/1040 [==============================] - 0s 100us/step - loss: 0.1686 - acc: 0.9923\n",
      "Epoch 12/30\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.1544 - acc: 0.9933\n",
      "Epoch 13/30\n",
      "1040/1040 [==============================] - 0s 119us/step - loss: 0.1415 - acc: 0.9942\n",
      "Epoch 14/30\n",
      "1040/1040 [==============================] - 0s 116us/step - loss: 0.1291 - acc: 0.9971\n",
      "Epoch 15/30\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.1180 - acc: 0.9981\n",
      "Epoch 16/30\n",
      "1040/1040 [==============================] - 0s 111us/step - loss: 0.1082 - acc: 0.9981\n",
      "Epoch 17/30\n",
      "1040/1040 [==============================] - 0s 115us/step - loss: 0.0989 - acc: 0.9981\n",
      "Epoch 18/30\n",
      "1040/1040 [==============================] - 0s 120us/step - loss: 0.0905 - acc: 0.9981\n",
      "Epoch 19/30\n",
      "1040/1040 [==============================] - 0s 99us/step - loss: 0.0830 - acc: 0.9981\n",
      "Epoch 20/30\n",
      "1040/1040 [==============================] - 0s 96us/step - loss: 0.0762 - acc: 0.9981\n",
      "Epoch 21/30\n",
      "1040/1040 [==============================] - 0s 98us/step - loss: 0.0701 - acc: 0.9981\n",
      "Epoch 22/30\n",
      "1040/1040 [==============================] - 0s 110us/step - loss: 0.0646 - acc: 0.9981\n",
      "Epoch 23/30\n",
      "1040/1040 [==============================] - 0s 134us/step - loss: 0.0597 - acc: 0.9981\n",
      "Epoch 24/30\n",
      "1040/1040 [==============================] - 0s 118us/step - loss: 0.0553 - acc: 0.9990\n",
      "Epoch 25/30\n",
      "1040/1040 [==============================] - 0s 97us/step - loss: 0.0512 - acc: 0.9990\n",
      "Epoch 26/30\n",
      "1040/1040 [==============================] - 0s 99us/step - loss: 0.0476 - acc: 0.9990\n",
      "Epoch 27/30\n",
      "1040/1040 [==============================] - 0s 97us/step - loss: 0.0443 - acc: 0.9990\n",
      "Epoch 28/30\n",
      "1040/1040 [==============================] - 0s 102us/step - loss: 0.0414 - acc: 0.9990\n",
      "Epoch 29/30\n",
      "1040/1040 [==============================] - 0s 97us/step - loss: 0.0387 - acc: 0.9990\n",
      "Epoch 30/30\n",
      "1040/1040 [==============================] - 0s 103us/step - loss: 0.0363 - acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1262734e0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513/513 [==============================] - 0s 124us/step\n",
      "Test accuracy: 0.783625730994152\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
