{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mark',\n",
       " 'twain',\n",
       " \"'\",\n",
       " 's',\n",
       " '!',\n",
       " 'hello',\n",
       " 'world',\n",
       " 'of',\n",
       " 'worlds',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'else',\n",
       " \"'\",\n",
       " 's']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "class Review():\n",
    "    '''\n",
    "    \n",
    "    Uses a tokenizer to separate a given string into individual \"tokens\". Each token is one word or symbol \n",
    "    (apostrophe, other punctuation) from the string\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.token_list = []\n",
    "        self.entities = []\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val)\n",
    "        self.tokenize()\n",
    "        self.generate_entities()\n",
    "        \n",
    "    def tokenize(self):\n",
    "        self.cleaned_txt = self.txt.lower()\n",
    "        self.cleaned_txt = nltk.word_tokenize(self.cleaned_txt)\n",
    "\n",
    "        \n",
    "        for item in self.cleaned_txt:\n",
    "            itemWords = re.split(\"(')\", item)\n",
    "            self.token_list.extend(itemWords)\n",
    "        \n",
    "        for word in self.token_list:\n",
    "            if word == \"\":\n",
    "                self.token_list.remove(word)\n",
    "                \n",
    "    def generate_entities(self):\n",
    "        fake = Entity(entity_type=\"author\", entity_start=0, entity_span=2)\n",
    "        self.entities.append(fake)\n",
    "        \n",
    "class Entity():\n",
    "    \"\"\"\n",
    "    Describes a sublist of a book review representing a named entity, such as an author, a title, or a publisher.\n",
    "    entity_type is a controlled vocabulary (author, title, publisher)\n",
    "    start and end arguments refer to the token list rather than the pre-tokenized text\n",
    "    entity_type=\"\", entity_start=0, entity_span=0, spacy_type=\"\", kb_id=\"\", kb_type=\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val) \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"<%s entity: string of %s tokens in length>\" % (self.entity_type, self.entity_span)\n",
    "        \n",
    "\n",
    "                \n",
    "#author1 = Entity(entity_type=\"author\", entity_start=0, entity_span=3)\n",
    "#author1.entity_start\n",
    "test = Review(txt=\"Mark Twain's! Hello world of worlds and everything else's\")\n",
    "test.token_list\n",
    "#test.token_list[test.entities[0].entity_start:test.entities[0].entity_start+test.entities[0].entity_span]\n",
    "#self.token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
