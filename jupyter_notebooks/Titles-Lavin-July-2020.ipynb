{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "- import various helpers, load data, select reviews by status and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from application.name_obj_classes import PubName, PersonName, remove_punct\n",
    "\n",
    "from application.review_obj_class import ReviewObj\n",
    "\n",
    "from application.text_preprocessing import preprocess_text\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.metrics import edit_distance\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *\n",
    "import database.models as models\n",
    "\n",
    "# load full text from db\n",
    "aps_details_single = models.Review().query.filter(models.Review.status.in_(('needs_crosscheck', 'done'))).filter(models.Review.review_type == 'single_focus').all()\n",
    "\n",
    "len(aps_details_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [i.reviewed_book_title for i in aps_details_single]\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_parsed = [ReviewObj(i.record_id, i.full_text) for i in aps_details_single]\n",
    "#reviews_parsed[0].cleaned_text\n",
    "reviews_parsed[0].cleaned_toks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_function_tail(sequence):\n",
    "    if sequence[-1].lower() in stopwords.words('english'):\n",
    "        sequence.pop()\n",
    "        return remove_function_tail(sequence)\n",
    "    else:\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates_tidy = []\n",
    "for one_review in reviews_parsed:\n",
    "    title_candidates = [list(),]\n",
    "    for token in one_review.cleaned_toks:\n",
    "        if token.istitle() or token in stopwords.words('english') or token in string.punctuation:\n",
    "            if len(title_candidates[-1]) > 0:\n",
    "                if token not in string.punctuation:\n",
    "                    title_candidates[-1].append(token)\n",
    "            else:\n",
    "                if token.istitle():\n",
    "                    title_candidates[-1].append(token)\n",
    "        else:\n",
    "            if len(title_candidates[-1]) > 0:\n",
    "                title_candidates.append(list())\n",
    "    \n",
    "    candidates_tidy = []\n",
    "    for sequence in title_candidates:\n",
    "        # rule out if all function words\n",
    "        all_function = True\n",
    "        for word in sequence:\n",
    "            if word.lower() not in stopwords.words('english'):\n",
    "                all_function = False\n",
    "                break\n",
    "        if all_function == False:\n",
    "            #remove function word tails recursively\n",
    "            sequence = remove_function_tail(sequence)\n",
    "            candidates_tidy.append(sequence)\n",
    "    all_candidates_tidy.append(candidates_tidy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "\n",
    "# loop all_candidates_tidy\n",
    "for e,t in enumerate(all_candidates_tidy):\n",
    "    # true label is titles[e]\n",
    "    # get headline text\n",
    "    block_one = aps_details_single[e].record_title\n",
    "    text_blocks = []\n",
    "    for i in t:\n",
    "        text_blocks.extend(i)\n",
    "    text_merged = [block_one + \" \" + \" \".join(text_blocks),]    \n",
    "    \n",
    "    # this title is comparison_set[-1]\n",
    "    comparison_set = titles + text_merged\n",
    "    \n",
    "    # compare \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(comparison_set)\n",
    "    vectors = X.toarray()\n",
    "    # loop all, get similarity, last one is always 1.0\n",
    "    scores = []\n",
    "    for v in vectors:\n",
    "        score = cosine_similarity([v], [vectors[-1]])\n",
    "        scores.append(score)\n",
    "    all_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often is the right answer the top answer? how often is the correct match in the top 5?\n",
    "output = []\n",
    "for e, score_grid in enumerate(all_scores):\n",
    "    df = pd.DataFrame()\n",
    "    df['score'] = [i[0][0] for i in score_grid][:-1]\n",
    "    df['title'] = titles\n",
    "    match = []\n",
    "    for i in range(len(titles)):\n",
    "        if e == i:\n",
    "            match.append('yes')\n",
    "        else:\n",
    "            match.append('no')\n",
    "    df['match'] = match\n",
    "    output.append(df.sort_values(by=\"score\", ascending=False).reset_index(drop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_match = []\n",
    "top_five = []\n",
    "top_ten = []\n",
    "top_25 = []\n",
    "for i in output:\n",
    "    df_five = i.iloc[:5]\n",
    "    df_ten = i.iloc[:10]\n",
    "    df_25 = i.iloc[:25]\n",
    "    top_match.append(i.iloc[0]['match'] == 'yes')\n",
    "    top_five.append(len(df_five.loc[df_five['match'] == 'yes']) > 0)\n",
    "    top_ten.append(len(df_ten.loc[df_ten['match'] == 'yes']) > 0)\n",
    "    top_25.append(len(df_25.loc[df_25['match'] == 'yes']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% correct without headline; 55.25% with\n",
    "len([i for i in top_match if i])/len(top_match)\n",
    "# 62%; 65.59%\n",
    "len([i for i in top_five if i])/len(top_five)\n",
    "# 65%; 68.98\n",
    "len([i for i in top_ten if i])/len(top_ten)\n",
    "#69%; 73.44\n",
    "len([i for i in top_25 if i])/len(top_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_audit = []\n",
    "for i in output:\n",
    "    df = i.loc[i['match'] == 'yes']\n",
    "    results_audit.append(df)\n",
    "# results_audit[10]\n",
    "# this title was 214th out of len(titles)\n",
    "df = pd.concat(results_audit).reset_index(drop=False)\n",
    "df = df.rename(columns={\"index\": \"original_index\", \"level_0\": \"rank\"})\n",
    "df.loc[df['rank'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#all_candidates_tidy[18]\n",
    "# top score is low\n",
    "# top 5 are basicaly tied\n",
    "\n",
    "variances = []\n",
    "top_score = []\n",
    "for i in output:\n",
    "    top_score.append(i.iloc[0]['score'])\n",
    "    v = np.var(list(i.iloc[:5]['score']))\n",
    "    variances.append(v)\n",
    "df['top_5_variance'] = variances\n",
    "df['top_score'] = top_score\n",
    "when_confident = list(df.loc[df['top_score'] > .40]['rank'])\n",
    "len([i for i in when_confident if i == 0])/len(when_confident), len(when_confident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "x = []\n",
    "y = []\n",
    "#graph accuracy as relates to confidence cutoffs\n",
    "for val in range(1, 100, 1):\n",
    "    c = val/100\n",
    "    when_confident = list(df.loc[df['top_score'] > c]['rank'])\n",
    "    try:\n",
    "        acc = len([i for i in when_confident if i == 0])/len(when_confident)\n",
    "        x.append(c)\n",
    "        y.append(acc)\n",
    "    except:\n",
    "        pass\n",
    "sns.lineplot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when_high_variance = list(df.loc[df['top_5_variance'] > .0009]['rank'])\n",
    "len([i for i in when_high_variance if i == 0])/len(when_high_variance ), len(when_high_variance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "#graph accuracy as relates to confidence cutoffs\n",
    "for val in range(10, 150, 2):\n",
    "    c = val/100000\n",
    "    when_high_variance = list(df.loc[df['top_5_variance'] > c]['rank'])\n",
    "    \n",
    "    try:\n",
    "        acc = len([i for i in when_high_variance  if i == 0])/len(when_high_variance )\n",
    "        x.append(c)\n",
    "        y.append(acc)\n",
    "    except:\n",
    "        pass\n",
    "sns.lineplot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how correlated are they? \n",
    "np.corrcoef(df['top_5_variance'], df['top_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a regression on these two variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# labels and matrix\n",
    "labels = [0 if i == 0 else 1 for i  in list(df['rank'])]\n",
    "len([i for i in labels if i == 0])/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(labels), len(list(df['top_5_variance'])), len(list(df['top_score']))\n",
    "Z = list(zip(list(df['top_5_variance']), list(df['top_score'].to_numpy())))\n",
    "# instantiate the model and fit to the training data\n",
    "\n",
    "lr = LogisticRegression()\n",
    "scores = cross_val_score(lr, Z, labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#62-65% is not very accurate ... these don't predict well enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(class_weight={0:.48, 1:.52})\n",
    "scores = cross_val_score(clf, Z, labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#72% is a little better ... and significantly better than 55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks \n",
    "\n",
    "None of these methods is very helpful. The problem is that very few title matches have high confidences. Most likely, the underlying test just isn't doing what I hoped. \n",
    "\n",
    "- Maybe combine fuzziness with vectors ... using either fuzzy clustering or word embeddings? Or doc2vec?\n",
    "- Maybe take out very common words from the candidates\n",
    "- TFIDF?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "words = []\n",
    "title_tokens = []\n",
    "for i in titles:\n",
    "    tokens = word_tokenize(i.lower())\n",
    "    title_tokens.append(tokens)\n",
    "    words.extend(tokens)\n",
    "words = list(set(words))\n",
    "\n",
    "data = [[x.lower() for x in word_tokenize(i.full_text)] for i in aps_details_single] + title_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=data, size=300, window=4, min_count=1, workers=8, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {}\n",
    "for w in words:\n",
    "    if len(w) > 1:\n",
    "        matches = model.wv.most_similar(w)\n",
    "        for m in matches:\n",
    "            if m[0] not in words:\n",
    "                if nltk.edit_distance(w,m[0]) < 1 and m[1] > 0.85:\n",
    "                    try:\n",
    "                        replacements[w].append(m[0])\n",
    "                    except:                        \n",
    "                        replacements[w] = [m[0],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.lower() for i in word_tokenize(aps_details_single[1].full_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace typos with common ocr list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
