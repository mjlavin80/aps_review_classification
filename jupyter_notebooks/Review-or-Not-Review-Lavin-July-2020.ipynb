{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "- import various helpers, load data, select reviews by status and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *\n",
    "import database.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full text from db\n",
    "aps_rows = models.Review().query.filter(models.Review.status.in_(('needs_crosscheck', 'needs_details', 'done'))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8569"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('/Volumes/TOSHIBA EXT/datasets/nyt_reviews_datastore.db')\n",
    "c = conn.cursor()\n",
    "# query nyt reviews and not\n",
    "nyt_rows = c.execute(\"SELECT * FROM metadata WHERE review_type IN ('not_review', 'multi', 'cluster', 'really_multi', 'single_focus')\").fetchall()\n",
    "len(nyt_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps_single_focus = [i for i in aps_rows if i.review_type == 'single_focus']\n",
    "len(aps_single_focus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In NYTBR section, Book Review or Not Book Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A NEW ESSAYIST.; C.F.G. Masterman, M.P., Criticises Kipling and Other British Institutions.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_rows[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4242, 4327)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_not_review = [i for i in nyt_rows if i[12] == 'not_review']\n",
    "nyt_review = [i for i in nyt_rows if i[12] in ('multi', 'cluster', 'really_multi', 'single_focus')]\n",
    "len(nyt_review), len(nyt_not_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_full_txt = [i[4] for i in nyt_review] + [i[4] for i in nyt_not_review]\n",
    "# make \"true labels\" (0s and 1s so scikit learn can score them)\n",
    "nyt_labels = [0 for i in range(len(nyt_review))] + [1 for i in range(len(nyt_not_review))]\n",
    "len(list_of_full_txt) == len(nyt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import various from scikit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logistic regression\n",
    "v = CountVectorizer(max_features)\n",
    "X = v.fit_transform(list_of_full_txt)\n",
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "# instantiate the model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lr, Z, nyt_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the rows into training data, training labels, test data, and test labels\n",
    "# test on 33% of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, nyt_labels, test_size=0.33, random_state=12)\n",
    "\n",
    "# fit to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make label predictions\n",
    "results = lr.predict(X_test)\n",
    "\n",
    "# generate probabilities for each label\n",
    "probs = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': {'f1': 0.884814942926323,\n",
       "  'precision': 0.8498338870431894,\n",
       "  'recall': 0.9227994227994228},\n",
       " 'not_review': {'f1': 0.8795660036166365,\n",
       "  'precision': 0.9191232048374905,\n",
       "  'recall': 0.8432732316227461},\n",
       " 'accuracy': 0.8822489391796322}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "# generate f1, precision, recall, and accuracy scores\n",
    "# I will discuss each of these in the lesson\n",
    "for y,z in [(\"review\",0),(\"not_review\",1)]:\n",
    "    scores[y] = {}\n",
    "    scores[y][\"f1\"] = f1_score(y_test, results, pos_label=z, average='binary')  \n",
    "    scores[y][\"precision\"] = precision_score(y_test, results, pos_label=z, average='binary')\n",
    "    scores[y][\"recall\"] = recall_score(y_test, results, pos_label=z, average='binary')\n",
    "\n",
    "scores[\"accuracy\"] = accuracy_score(y_test, results)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "coefs = []\n",
    "for key,val in v.vocabulary_.items():\n",
    "    terms.append(key)\n",
    "    coefs.append(lr.coef_[0][val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pp</td>\n",
       "      <td>-5.146061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>he</td>\n",
       "      <td>-3.361216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>-2.945701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>her</td>\n",
       "      <td>-2.848645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>volume</td>\n",
       "      <td>-2.781168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>she</td>\n",
       "      <td>-2.397521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>with</td>\n",
       "      <td>-2.388235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>tile</td>\n",
       "      <td>-2.328123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>reader</td>\n",
       "      <td>-2.254851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>12mo</td>\n",
       "      <td>-2.213153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>chapter</td>\n",
       "      <td>-2.054118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>his</td>\n",
       "      <td>-1.916618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>ill</td>\n",
       "      <td>-1.837196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>all</td>\n",
       "      <td>-1.799856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.725866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>pages</td>\n",
       "      <td>-1.540068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>history</td>\n",
       "      <td>-1.485525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.446554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>so</td>\n",
       "      <td>-1.433299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>which</td>\n",
       "      <td>-1.403423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>man</td>\n",
       "      <td>-1.399809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>had</td>\n",
       "      <td>-1.394639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.359994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>th</td>\n",
       "      <td>-1.329488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>their</td>\n",
       "      <td>-1.326744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>these</td>\n",
       "      <td>-1.275521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>cloth</td>\n",
       "      <td>-1.275065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>human</td>\n",
       "      <td>-1.205001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>life</td>\n",
       "      <td>-1.192520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>those</td>\n",
       "      <td>-1.191721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term      coef\n",
       "0        pp -5.146061\n",
       "1        he -3.361216\n",
       "2       and -2.945701\n",
       "3       her -2.848645\n",
       "4    volume -2.781168\n",
       "5       she -2.397521\n",
       "6      with -2.388235\n",
       "7      tile -2.328123\n",
       "8    reader -2.254851\n",
       "9      12mo -2.213153\n",
       "10  chapter -2.054118\n",
       "11      his -1.916618\n",
       "12      ill -1.837196\n",
       "13      all -1.799856\n",
       "14       50 -1.725866\n",
       "15    pages -1.540068\n",
       "16  history -1.485525\n",
       "17       is -1.446554\n",
       "18       so -1.433299\n",
       "19    which -1.403423\n",
       "20      man -1.399809\n",
       "21      had -1.394639\n",
       "22       of -1.359994\n",
       "23       th -1.329488\n",
       "24    their -1.326744\n",
       "25    these -1.275521\n",
       "26    cloth -1.275065\n",
       "27    human -1.205001\n",
       "28     life -1.192520\n",
       "29    those -1.191721"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block produces a dataframe with the top 30 terms associated with label 0\n",
    "df_coef = pd.DataFrame()\n",
    "df_coef['term'] = terms\n",
    "df_coef['coef'] = coefs\n",
    "df_coef = df_coef.sort_values(by='coef').reset_index(drop=True)\n",
    "df_coef.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>408132</td>\n",
       "      <td>literary</td>\n",
       "      <td>1.966858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408133</td>\n",
       "      <td>number</td>\n",
       "      <td>2.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408134</td>\n",
       "      <td>has</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408135</td>\n",
       "      <td>publishers</td>\n",
       "      <td>2.025496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408136</td>\n",
       "      <td>magazine</td>\n",
       "      <td>2.029833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408137</td>\n",
       "      <td>yesterday</td>\n",
       "      <td>2.068803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408138</td>\n",
       "      <td>performance</td>\n",
       "      <td>2.113786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408139</td>\n",
       "      <td>l2mo</td>\n",
       "      <td>2.126622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408140</td>\n",
       "      <td>london</td>\n",
       "      <td>2.154999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408141</td>\n",
       "      <td>times</td>\n",
       "      <td>2.157221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408142</td>\n",
       "      <td>article</td>\n",
       "      <td>2.236704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408143</td>\n",
       "      <td>at</td>\n",
       "      <td>2.278984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408144</td>\n",
       "      <td>saturday</td>\n",
       "      <td>2.279148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408145</td>\n",
       "      <td>last</td>\n",
       "      <td>2.508893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408146</td>\n",
       "      <td>music</td>\n",
       "      <td>2.538258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408147</td>\n",
       "      <td>me</td>\n",
       "      <td>2.578978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408148</td>\n",
       "      <td>exhibition</td>\n",
       "      <td>2.592475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408149</td>\n",
       "      <td>readers</td>\n",
       "      <td>2.731123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408150</td>\n",
       "      <td>review</td>\n",
       "      <td>2.770367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408151</td>\n",
       "      <td>poem</td>\n",
       "      <td>2.777325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408152</td>\n",
       "      <td>issue</td>\n",
       "      <td>2.793155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408153</td>\n",
       "      <td>in</td>\n",
       "      <td>2.972769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408154</td>\n",
       "      <td>new</td>\n",
       "      <td>3.092456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408155</td>\n",
       "      <td>be</td>\n",
       "      <td>3.135520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408156</td>\n",
       "      <td>week</td>\n",
       "      <td>3.209241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408157</td>\n",
       "      <td>play</td>\n",
       "      <td>3.286714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408158</td>\n",
       "      <td>published</td>\n",
       "      <td>3.714079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408159</td>\n",
       "      <td>by</td>\n",
       "      <td>4.858057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408160</td>\n",
       "      <td>books</td>\n",
       "      <td>4.922154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408161</td>\n",
       "      <td>will</td>\n",
       "      <td>5.045089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               term      coef\n",
       "408132     literary  1.966858\n",
       "408133       number  2.005087\n",
       "408134          has  2.006700\n",
       "408135   publishers  2.025496\n",
       "408136     magazine  2.029833\n",
       "408137    yesterday  2.068803\n",
       "408138  performance  2.113786\n",
       "408139         l2mo  2.126622\n",
       "408140       london  2.154999\n",
       "408141        times  2.157221\n",
       "408142      article  2.236704\n",
       "408143           at  2.278984\n",
       "408144     saturday  2.279148\n",
       "408145         last  2.508893\n",
       "408146        music  2.538258\n",
       "408147           me  2.578978\n",
       "408148   exhibition  2.592475\n",
       "408149      readers  2.731123\n",
       "408150       review  2.770367\n",
       "408151         poem  2.777325\n",
       "408152        issue  2.793155\n",
       "408153           in  2.972769\n",
       "408154          new  3.092456\n",
       "408155           be  3.135520\n",
       "408156         week  3.209241\n",
       "408157         play  3.286714\n",
       "408158    published  3.714079\n",
       "408159           by  4.858057\n",
       "408160        books  4.922154\n",
       "408161         will  5.045089"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view the top 30 terms associated with label 1, we look at the bottom rows of the same dataframe\n",
    "#I2mo ... pub announcements have both, but OCR may be of a lower quality with long lists and blurb announcements\n",
    "#or just more hances to get it wrong\n",
    "df_coef.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LATEST PUBLICATIONS  Books Received During the Week Ended July 25 Classified and Annotated According'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I2mos = [i for i in nyt_not_review if 'I2mo' in i[4]]\n",
    "I2mos[1][4][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793947198969736"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run on aps_reviews\n",
    "# make predictions using nonbinary data \n",
    "aps_reviews = [i.full_text for i in aps_rows if i.review_type in ('single_focus', 'multi', 'cluster')]\n",
    "aps_ids = [i.record_id for i in aps_rows if i.review_type in ('single_focus', 'multi', 'cluster')]\n",
    "aps_urls = [\"https://aps-web-app.matthew-lavin.com/static/pdf/%s.pdf\"%i for i in aps_ids]\n",
    "\n",
    "aps_vectors = v.transform(aps_reviews)\n",
    "aps_tfidf = tfidf.fit_transform(aps_vectors)\n",
    "\n",
    "# generate probabilities for each label\n",
    "aps_probs = lr.predict_proba(aps_tfidf)\n",
    "\n",
    "#display the results as a pandas dataframe\n",
    "aps_results = pd.DataFrame()\n",
    "\n",
    "# make columns for the original label, the nyt_id, the cluster_id, the pdf url, and the predicted probabilities\n",
    "\n",
    "aps_results['aps_id'] = aps_ids\n",
    "aps_results['url'] = aps_urls\n",
    "aps_results['prob_review'] = [i[0] for i in aps_probs]\n",
    "aps_results['prob_not_review'] = [i[1] for i in aps_probs]\n",
    "len(aps_results.loc[aps_results['prob_review'] > 0.5].reset_index())/len(aps_results)\n",
    "#79.39% of aps reviews have a naive probability score over .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5893719806763285"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this doesn't tell us how many false positives we might get, just that a model trained on NYT reviews usually recognizes APS reviews as reviews\n",
    "aps_not_reviews = [i.full_text for i in aps_rows if i.review_type == 'not_review']\n",
    "aps_non_review_ids = [i.record_id for i in aps_rows if i.review_type == 'not_review']\n",
    "aps_non_review_urls = [\"https://aps-web-app.matthew-lavin.com/static/pdf/%s.pdf\"%i for i in aps_non_review_ids]\n",
    "\n",
    "aps_non_review_vectors = v.transform(aps_not_reviews)\n",
    "aps_non_review_tfidf = tfidf.fit_transform(aps_non_review_vectors)\n",
    "\n",
    "# generate probabilities for each label\n",
    "aps_non_review_probs = lr.predict_proba(aps_non_review_tfidf)\n",
    "\n",
    "#display the results as a pandas dataframe\n",
    "aps_non_review_results = pd.DataFrame()\n",
    "\n",
    "# make columns for the original label, the nyt_id, the cluster_id, the pdf url, and the predicted probabilities\n",
    "\n",
    "aps_non_review_results['aps_id'] = aps_non_review_ids\n",
    "aps_non_review_results['url'] = aps_non_review_urls\n",
    "aps_non_review_results['prob_review'] = [i[0] for i in aps_non_review_probs]\n",
    "aps_non_review_results['prob_not_review'] = [i[1] for i in aps_non_review_probs]\n",
    "len(aps_non_review_results.loc[aps_non_review_results['prob_not_review'] > 0.5].reset_index())/len(aps_non_review_results)\n",
    "# 58.93% of non-reviews would have a non-review probability over 50%, so we might want to adjust to reduce false positives \n",
    "# However, say we started with a mix of 80/20 reviews and not reviews\n",
    "# If we got these results with 1000 objects, we would have 635 true postives, 165 false negatives, 118 true negatives and 82 false positives\n",
    "# If this were all true, we'd be running calculations on a sample that's 88.5% book reviews and 11.5% not\n",
    "# Pretty good, but we want better, especially the false positive\n",
    "# Option 1: improve the model with data, setup, or learning method (labor)\n",
    "# Option 2: raise the probability threshold to be considered a review (also creates more false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "\n",
    "- As the number of models evaluated on the same data increases, the odds of model performance be good just by chance goes up. This is not as straightforward as a p-value with multiple hypotheses, but it needs to be considered.\n",
    "- As a result, this is exploratory, and models should be validated against separate datasets in the future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, Z, nyt_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lasso = linear_model.Lasso()\n",
    "scores = cross_val_score(lasso, Z, nyt_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(neigh, Z, nyt_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='sigmoid')\n",
    "scores = cross_val_score(svm, Z, nyt_labels, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "v = CountVectorizer(max_features=10000)\n",
    "X = v.fit_transform(list_of_full_txt)\n",
    "v.vocabulary_\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(10000,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, nyt_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(10000,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, nyt_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5741/5741 [==============================] - 2s 301us/step - loss: 0.6864 - acc: 0.7300\n",
      "Epoch 2/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.6601 - acc: 0.8074\n",
      "Epoch 3/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.6238 - acc: 0.8363\n",
      "Epoch 4/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.5796 - acc: 0.8420\n",
      "Epoch 5/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.5294 - acc: 0.8627\n",
      "Epoch 6/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.4761 - acc: 0.8669\n",
      "Epoch 7/20\n",
      "5741/5741 [==============================] - 1s 89us/step - loss: 0.4238 - acc: 0.8857\n",
      "Epoch 8/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.3754 - acc: 0.8948\n",
      "Epoch 9/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.3326 - acc: 0.9058\n",
      "Epoch 10/20\n",
      "5741/5741 [==============================] - 1s 90us/step - loss: 0.2954 - acc: 0.9159\n",
      "Epoch 11/20\n",
      "5741/5741 [==============================] - 1s 89us/step - loss: 0.2636 - acc: 0.9239\n",
      "Epoch 12/20\n",
      "5741/5741 [==============================] - 1s 101us/step - loss: 0.2364 - acc: 0.9335\n",
      "Epoch 13/20\n",
      "5741/5741 [==============================] - 1s 89us/step - loss: 0.2123 - acc: 0.9410\n",
      "Epoch 14/20\n",
      "5741/5741 [==============================] - 1s 95us/step - loss: 0.1920 - acc: 0.9477\n",
      "Epoch 15/20\n",
      "5741/5741 [==============================] - 1s 100us/step - loss: 0.1736 - acc: 0.9547\n",
      "Epoch 16/20\n",
      "5741/5741 [==============================] - 1s 89us/step - loss: 0.1576 - acc: 0.9613\n",
      "Epoch 17/20\n",
      "5741/5741 [==============================] - 1s 102us/step - loss: 0.1433 - acc: 0.9659\n",
      "Epoch 18/20\n",
      "5741/5741 [==============================] - 1s 89us/step - loss: 0.1306 - acc: 0.9704\n",
      "Epoch 19/20\n",
      "5741/5741 [==============================] - 1s 93us/step - loss: 0.1191 - acc: 0.9740\n",
      "Epoch 20/20\n",
      "5741/5741 [==============================] - 1s 88us/step - loss: 0.1084 - acc: 0.9779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12d863748>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2828/2828 [==============================] - 1s 233us/step\n",
      "Test accuracy: 0.8988684583587053\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5741/5741 [==============================] - 4s 650us/step - loss: 0.6308 - acc: 0.6459\n",
      "Epoch 2/30\n",
      "5741/5741 [==============================] - 2s 412us/step - loss: 0.5468 - acc: 0.7215\n",
      "Epoch 3/30\n",
      "5741/5741 [==============================] - 2s 416us/step - loss: 0.5292 - acc: 0.7333\n",
      "Epoch 4/30\n",
      "5741/5741 [==============================] - 2s 413us/step - loss: 0.5227 - acc: 0.7391\n",
      "Epoch 5/30\n",
      "5741/5741 [==============================] - 2s 427us/step - loss: 0.5205 - acc: 0.7396\n",
      "Epoch 6/30\n",
      "5741/5741 [==============================] - 2s 409us/step - loss: 0.5175 - acc: 0.7419\n",
      "Epoch 7/30\n",
      "5741/5741 [==============================] - 3s 442us/step - loss: 0.5147 - acc: 0.7399\n",
      "Epoch 8/30\n",
      "5741/5741 [==============================] - 3s 476us/step - loss: 0.5146 - acc: 0.7433 0s - loss: 0.5123 - acc\n",
      "Epoch 9/30\n",
      "5741/5741 [==============================] - 2s 417us/step - loss: 0.5122 - acc: 0.7431\n",
      "Epoch 10/30\n",
      "5741/5741 [==============================] - 2s 433us/step - loss: 0.5109 - acc: 0.7455\n",
      "Epoch 11/30\n",
      "5741/5741 [==============================] - 3s 566us/step - loss: 0.5088 - acc: 0.7471\n",
      "Epoch 12/30\n",
      "5741/5741 [==============================] - 3s 484us/step - loss: 0.5062 - acc: 0.7478\n",
      "Epoch 13/30\n",
      "5741/5741 [==============================] - 3s 597us/step - loss: 0.5041 - acc: 0.7518\n",
      "Epoch 14/30\n",
      "5741/5741 [==============================] - 3s 472us/step - loss: 0.5031 - acc: 0.7514\n",
      "Epoch 15/30\n",
      "5741/5741 [==============================] - 3s 535us/step - loss: 0.5014 - acc: 0.7532\n",
      "Epoch 16/30\n",
      "5741/5741 [==============================] - 4s 752us/step - loss: 0.5012 - acc: 0.7553\n",
      "Epoch 17/30\n",
      "5741/5741 [==============================] - 3s 515us/step - loss: 0.4978 - acc: 0.7542\n",
      "Epoch 18/30\n",
      "5741/5741 [==============================] - 3s 475us/step - loss: 0.4955 - acc: 0.7588\n",
      "Epoch 19/30\n",
      "5741/5741 [==============================] - 3s 553us/step - loss: 0.4944 - acc: 0.7574\n",
      "Epoch 20/30\n",
      "5741/5741 [==============================] - 2s 400us/step - loss: 0.4926 - acc: 0.7581\n",
      "Epoch 21/30\n",
      "5741/5741 [==============================] - 3s 463us/step - loss: 0.4903 - acc: 0.7621\n",
      "Epoch 22/30\n",
      "5741/5741 [==============================] - 2s 407us/step - loss: 0.4895 - acc: 0.7603\n",
      "Epoch 23/30\n",
      "5741/5741 [==============================] - 3s 487us/step - loss: 0.4893 - acc: 0.7619\n",
      "Epoch 24/30\n",
      "5741/5741 [==============================] - 2s 423us/step - loss: 0.4863 - acc: 0.7593\n",
      "Epoch 25/30\n",
      "5741/5741 [==============================] - 2s 414us/step - loss: 0.4855 - acc: 0.7615\n",
      "Epoch 26/30\n",
      "5741/5741 [==============================] - 3s 462us/step - loss: 0.4859 - acc: 0.7612\n",
      "Epoch 27/30\n",
      "5741/5741 [==============================] - 2s 404us/step - loss: 0.4845 - acc: 0.7636\n",
      "Epoch 28/30\n",
      "5741/5741 [==============================] - 2s 432us/step - loss: 0.4827 - acc: 0.7648\n",
      "Epoch 29/30\n",
      "5741/5741 [==============================] - 3s 483us/step - loss: 0.4816 - acc: 0.7682\n",
      "Epoch 30/30\n",
      "5741/5741 [==============================] - 2s 389us/step - loss: 0.4790 - acc: 0.7638\n",
      "2828/2828 [==============================] - 0s 154us/step\n",
      "Test accuracy: 0.7142857142014079\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "v = CountVectorizer(max_features=30)\n",
    "X = v.fit_transform(list_of_full_txt)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "Z = tfidf.fit_transform(X)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(30,)),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Z, nyt_labels, test_size=0.33, random_state=14)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=5)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
