{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *\n",
    "import database.models as models\n",
    "\n",
    "# load full text from db\n",
    "aps_details_single = models.Review().query.filter(models.Review.status.in_(('needs_crosscheck', 'done'))).filter(models.Review.review_type == 'single_focus').all()\n",
    "\n",
    "len(aps_details_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [i.reviewed_book_title for i in aps_details_single]\n",
    "authors = [i.reviewed_author_name for i in aps_details_single]\n",
    "years = [i.year for i in aps_details_single]\n",
    "publishers = [i.reviewed_book_publisher for i in aps_details_single]\n",
    "author_viafs = [i.reviewed_author_viaf_match for i in aps_details_single ]\n",
    "publisher_viafs = [i.reviewed_book_publisher_viaf_match for i in aps_details_single]\n",
    "known_publishers = list(set([i for i in publishers if i != '' and i is not None]))\n",
    "len(known_publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzysearch import find_near_matches\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "def is_word(word):\n",
    "    \"\"\"\n",
    "    Returns true if word is found in sym_spell dictionary, otherwise returns false.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fake = sym_spell._words[word]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_surname(surname):\n",
    "    \"\"\"\n",
    "    Returns true if name is found in sym_spell author surname dictionary, otherwise returns false.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fake = author_surname_dict._words[surname.lower()]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fix_hyphenated_words(toks):\n",
    "    \"\"\"\n",
    "    Replaces hyphenated words with single word.\n",
    "    \"\"\"\n",
    "    dash_indices = find_dashes(toks)\n",
    "    to_be_deleted = []\n",
    "    for i in dash_indices:\n",
    "        #if neither are words, e.g. pieces of names or misspellings\n",
    "        if (is_word(toks[i][:-1])==False or is_word(toks[i+1])==False):\n",
    "            #replace first item with combined, delete second item\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "            #if combined is a word\n",
    "        elif (is_word((toks[i][:-1] + toks[i+1]))):\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "        elif (is_surname((toks[i][:-1] + toks[i+1]))):\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "        else:\n",
    "            pass\n",
    "    #do this after so u don't heck up the indices\n",
    "    toks = [w for i, w in enumerate(toks) if i not in to_be_deleted]\n",
    "    return toks\n",
    "\n",
    "def find_dashes(toks):\n",
    "    \"\"\"\n",
    "    Returns list of indices for words ending in dashes.\n",
    "    \"\"\"\n",
    "    dash_indices = [i for i, word in enumerate(toks) if (len(word)>1) and (word.endswith('-'))]\n",
    "    return dash_indices\n",
    "\n",
    "def remove_function_head(sequence):\n",
    "    if sequence:\n",
    "        if sequence[0][0].islower():\n",
    "            sequence.pop(0)\n",
    "            return remove_function_head(sequence)\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def remove_function_tail(sequence):\n",
    "    if sequence: \n",
    "        if sequence[-1].lower() in stopwords.words('english'):\n",
    "            sequence.pop()\n",
    "            return remove_function_tail(sequence)\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_honorifics(sequence):\n",
    "    honorifics = \"\"\"Doctor,Dr,Mr,Mrs,Miss,Msgr,Monsignor,Rev,Reverend,Hon,Honorable,Honourable,Prof,Professor,Madame,Madam,Lady,Lord,Sir,Dame,Master,Mistress,Princess,Prince,Duke,Duchess,Baron,Father,Chancellor,Principal,President,Pres,Warden,Dean,Regent,Rector,Provost,Director\"\"\"\n",
    "    honorific_list = honorifics.lower().split(',')\n",
    "    if sequence:\n",
    "        if len(sequence) == 1 and sequence[0].lower() in honorific_list:\n",
    "            return None\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def cull_title(text_block, patterns):\n",
    "    for i in patterns:\n",
    "        text_block = text_block.replace(i, \"\")\n",
    "    culled_title_candidates = [list(),]\n",
    "    for token in word_tokenize(text_block):\n",
    "        if token[0].isupper() or token.lower() in stopwords.words('english') or token in string.punctuation:\n",
    "            if len(culled_title_candidates[-1]) > 0:\n",
    "                if token not in string.punctuation:\n",
    "                    culled_title_candidates[-1].append(token)\n",
    "            else:\n",
    "                if token[0].isupper():\n",
    "                    culled_title_candidates[-1].append(token)\n",
    "        else:\n",
    "            if len(culled_title_candidates[-1]) > 0:\n",
    "                culled_title_candidates.append(list())\n",
    "    # remove any culled_title_candidate if it's just an honorific\n",
    "    candidates_tidy = []\n",
    "    for sequence in culled_title_candidates:\n",
    "        \n",
    "        if len(sequence) > 0:\n",
    "            #remove lowercase function word heads and tails recursively\n",
    "            sequence = remove_function_tail(sequence)\n",
    "            sequence = remove_function_head(sequence)\n",
    "            sequence = remove_honorifics(sequence)\n",
    "            if sequence:\n",
    "                \n",
    "                candidates_tidy.append(\" \".join(sequence).lower())\n",
    "    return candidates_tidy\n",
    "\n",
    "class ReviewObject():\n",
    "    \"\"\"\n",
    "    object class for a book review, including variables for labels, full text, parsed entities, match data, etc.\n",
    "    tokens_raw (list)\n",
    "    tokens_tidy (list)\n",
    "    title_candidates {candidate_string: score, candidate_string: score}\n",
    "    author_candidates {candidate_string: score, candidate_string: score}\n",
    "    publisher_candidates {candidate_string: score, candidate_string: score}\n",
    "    candidate_mappings {\"authors\": { candidate_string: {match_string: match_score, match_string: match_score}}, \"titles\": None, \"publishers\": None}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, title, full_text, **kwargs):\n",
    "        # instantiate with metdata from db\n",
    "        self.title = title\n",
    "        self.full_text = full_text\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.make_tokens()\n",
    "        self.make_tidy()\n",
    "        self.extract_title_candidates()\n",
    "        self.extract_author_candidates()\n",
    "        self.extract_publisher_candidates()\n",
    "        self.map_candidates_to_entities()\n",
    "        self.select_top_matches()\n",
    "        \n",
    "    def make_tokens(self):\n",
    "        self.tokens_raw = word_tokenize(self.full_text)\n",
    "\n",
    "    def make_tidy(self):\n",
    "        #adding space around certain problem punctuation\n",
    "        txt = re.sub(',',' , ',self.full_text)\n",
    "        txt = re.sub(';',' ; ',txt)\n",
    "        txt = re.sub(':',' : ',txt)\n",
    "        txt = re.sub('\"',' \" ',txt)\n",
    "        txt = re.sub('&',' & ',txt)\n",
    "        txt = re.sub(\"'(?!s)\",\" ' \" ,txt)\n",
    "        # remove extra whitespace\n",
    "        txt = re.sub(' +',' ',txt)\n",
    "        # fix hyphenated words\n",
    "        txt = ' '.join(fix_hyphenated_words(txt.split()))\n",
    "        #putting space back\n",
    "        txt = re.sub(' , ',', ',txt)\n",
    "        txt = re.sub(' ; ','; ',txt)\n",
    "        txt = re.sub(' : ',': ',txt)\n",
    "        #and fixing hyphen issues\n",
    "        txt = re.sub('-(?!\\w)',' - ',txt)\n",
    "        self.text_tidy = re.sub('(?<!\\w)-',' - ',txt)\n",
    "        self.tokens_tidy = word_tokenize(self.text_tidy)\n",
    "        \n",
    "        self.text_tidy_lower = self.text_tidy.lower()\n",
    "        self.tokens_tidy_lower = word_tokenize(self.text_tidy_lower)\n",
    "        \n",
    "    \n",
    "    def extract_title_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Look for capitalized string in review title\n",
    "        2. Look for before and after cues, get capitalized strings\n",
    "        3. If no cues, look for capitalized strings\n",
    "        4. Cull obvious false positives \n",
    "        \"\"\"\n",
    "        patterns = [\"chapter of\", \"chapters of\", \"latest\", \"book called\", \"volume called\", \"novel called\", \"volume of\", \"edition of\", \"novel\", \\\n",
    "                    \"study of\", \"entitled\", \"the author of\", \"with the title\", \"the manner in which\" \\\n",
    "                    \"book\", \"story\", \"life of\"]\n",
    "        \n",
    "        title_candidates = []\n",
    "        if self.record_title:\n",
    "            title_candidates.extend(re.findall(\"\\\".+\\\"\", self.record_title))\n",
    "            title_candidates.extend(re.findall(\"\\'.+\\'\", self.record_title))\n",
    "            \n",
    "        culled_title_candidates_all = []\n",
    "        for text_block in title_candidates:\n",
    "            culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "                \n",
    "        if len(culled_title_candidates_all) == 0:\n",
    "            title_candidates.append(self.record_title)\n",
    "            culled_title_candidates_all = []\n",
    "            for text_block in title_candidates:\n",
    "                culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "\n",
    "            for i in patterns:\n",
    "                pattern = \"\".join([i, \".+?\\.\"])\n",
    "                title_candidates.extend(re.findall(pattern, self.text_tidy))\n",
    "\n",
    "\n",
    "            for text_block in title_candidates:\n",
    "                culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "        \n",
    "        self.title_candidates = Counter(culled_title_candidates_all)\n",
    "    \n",
    "    def extract_author_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Look for honorifics, try to extrapolate surnames\n",
    "        2. Look for surnames in review title\n",
    "        2. Look for before and after cues + surnames in text\n",
    "        3. If no cues, look for capitalized N-grams ending with surnames\n",
    "        4. Score each candidate entry based on how it was found\n",
    "        \"\"\"\n",
    "        titles = \"\"\"Doctor,Dr,Mr,Mrs,Miss,Msgr,Monsignor,Rev,Reverend,Hon,Honorable,Honourable,Prof,Professor,Madame,Madam,Lady,Lord,Sir,Dame,Master,Mistress,Princess,Prince,Duke,Duchess,Baron,Father,Chancellor,Principal,President,Pres,Warden,Dean,Regent,Rector,Provost,Director\"\"\"\n",
    "        titles = titles.split(',')\n",
    "\n",
    "        full_names = {}\n",
    "\n",
    "        for e,i in enumerate(self.tokens_tidy):\n",
    "            maybe_title = \"\".join([z for z in i if z.isalpha()])\n",
    "            if maybe_title in titles:\n",
    "\n",
    "                surname = []\n",
    "                for p in [e+1, e+2, e+3]:\n",
    "                    try:\n",
    "                        if text[p][0].isupper():\n",
    "                            surname.append(text[p])\n",
    "                    except:\n",
    "                        pass\n",
    "                if len(surname) > 0:\n",
    "                    surname = \" \".join(surname).replace(\"'s\", \"\")\n",
    "                    surname_cleaned = []\n",
    "                    for s in surname:\n",
    "                        if s not in '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~':\n",
    "                            surname_cleaned.append(s)\n",
    "                    surname_cleaned = \"\".join(surname_cleaned)\n",
    "                    try:\n",
    "                        check = full_names[surname]\n",
    "                    except:\n",
    "                        full_names[surname] = {}\n",
    "                    try:\n",
    "                        full_names[surname]['title'].append(maybe_title)\n",
    "                    except:\n",
    "                        full_names[surname]['title'] = [maybe_title,]\n",
    "                    try:\n",
    "                        full_names[surname]['surname_cleaned'].append(surname_cleaned)\n",
    "                    except:\n",
    "                        full_names[surname]['surname_cleaned'] = [surname_cleaned,]\n",
    "\n",
    "        for surname in full_names.keys():\n",
    "            s = surname.split()\n",
    "            for e, i in enumerate(text):\n",
    "                if text[e:e+len(s)] == s:\n",
    "                    forename = \"\".join([x for x in text[e-1] if x.isalpha()])\n",
    "                    if forename.istitle() and forename not in titles:\n",
    "                        try:\n",
    "                            full_names[surname]['forename'].append(forename)\n",
    "                        except:\n",
    "                            full_names[surname]['forename'] = [forename,]\n",
    "                try:\n",
    "                    forenames = full_names[surname]['forename']\n",
    "                except:\n",
    "                    full_names[surname]['forename'] = []\n",
    "\n",
    "        for name in full_names.keys():\n",
    "            for i in full_names[name]['forename']:\n",
    "                try: \n",
    "                    full_names[name]['full_name'].append(i + \" \" + name)\n",
    "                except:\n",
    "                    full_names[name]['full_name'] = [i + \" \" + name,]\n",
    "            try:\n",
    "                full = full_names[surname]['full_name']\n",
    "            except:\n",
    "                full_names[name]['full_name'] = []\n",
    "        \n",
    "        full_name_candidates = {}\n",
    "\n",
    "        for n in full_names.keys():\n",
    "            for f in full_names[n]['full_name']:\n",
    "                try:\n",
    "                    full_name_candidates[f] += 1\n",
    "                except:\n",
    "                    full_name_candidates[f] = 1\n",
    "        \n",
    "        author_surname_candidates = {}\n",
    "        \n",
    "        # add title and surnames\n",
    "        for n,o in full_names.items():\n",
    "            for i in o['surname_cleaned']:\n",
    "                try:\n",
    "                    author_surname_candidates[i] +=1\n",
    "                except:\n",
    "                    author_surname_candidates[i] =1\n",
    "                \n",
    "                # check if surname in a full name\n",
    "                name_part = False\n",
    "                for full in full_name_candidates.keys():\n",
    "                    if i in full:\n",
    "                        name_part = True\n",
    "                if not name_part:\n",
    "                    try: \n",
    "                        full_name_candidates[i] += 1\n",
    "                    except:\n",
    "                        full_name_candidates[i] = 1\n",
    "                        \n",
    "        self.author_candidates = full_name_candidates\n",
    "        self.author_surname_candidates = author_surname_candidates\n",
    "    \n",
    "    def extract_publisher_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Fuzzy match against known publishers, and count mentions\n",
    "        2. If no matches, look for pub ends and capitalization, and count each candidate\n",
    "        \"\"\"\n",
    "        pub_ends = ['company','co','incorporated','inc','firm','press','group','publishers','publishing', \\\n",
    "            'publications','pub','books','ltd','limited','society','house','associates', 'book', 'university']\n",
    "        \n",
    "        self.publisher_candidates = {}\n",
    "        \n",
    "        #this list is defined outside the class\n",
    "        for p in known_publishers:\n",
    "            #base fuzziness on length of pubname\n",
    "            if len(p) < 6:\n",
    "                fuzz=0\n",
    "            elif len(p) > 5 and len(p) < 10:\n",
    "                fuzz=1\n",
    "            elif len(p) > 9 and len(p) < 15:\n",
    "                fuzz=2\n",
    "            else:\n",
    "                fuzz=3\n",
    "            \n",
    "            matches = find_near_matches(p, self.text_tidy, max_l_dist=fuzz)\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                match_strings = [self.text_tidy[m.start:m.end] for m in matches]\n",
    "                for i in match_strings:\n",
    "                    try:\n",
    "                        self.publisher_candidates[p] += 1\n",
    "                    except:\n",
    "                        self.publisher_candidates[p] = 1\n",
    "                        \n",
    "        if self.publisher_candidates == {}:\n",
    "            publisher_candidates = [list(),]\n",
    "            for token in self.tokens_tidy:\n",
    "                if token[0].isupper() or token in ['and', '&'] or token in string.punctuation:\n",
    "                    if len(publisher_candidates[-1]) > 0:\n",
    "                        if token not in string.punctuation:\n",
    "                            publisher_candidates[-1].append(token) \n",
    "                    else:\n",
    "                        if token.istitle():\n",
    "                            publisher_candidates[-1].append(token)\n",
    "                else:\n",
    "                    if len(publisher_candidates[-1]) > 0:\n",
    "                        publisher_candidates.append(list())\n",
    "    \n",
    "            matches = []\n",
    "            for sequence in publisher_candidates:\n",
    "                for token in sequence:\n",
    "                    normed_token = token.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "                    if normed_token in pub_ends:\n",
    "                        matches.append(\" \".join(sequence))\n",
    "                        break\n",
    "                        \n",
    "            self.publisher_candidates = Counter(matches)\n",
    "\n",
    "    def map_candidates_to_entities(self):\n",
    "        \"\"\"\n",
    "        1. Set keys with no values if no candidates \n",
    "        2. Map and score publisher matches\n",
    "        3. Map and score author matches\n",
    "        4. Map and score title matches\n",
    "        5. Map and score whole book matches\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def select_top_matches(self):\n",
    "        \"\"\"\n",
    "        Return one publisher, one title, and one or more authors thought to be the correct matches \n",
    "        \"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Review Object titled '%s' with the following instance variables: %s \" % (self.title, \"'\"+\"', '\".join(self.__dict__.keys())+\"'\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from application.name_obj_classes import PubName, PersonName, remove_punct\n",
    "\n",
    "from application.review_obj_class import ReviewObj\n",
    "\n",
    "from application.text_preprocessing import preprocess_text\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.metrics import edit_distance\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_objects = []\n",
    "for i in aps_details_single:\n",
    "    r = ReviewObject(title=i.reviewed_book_title, full_text=i.full_text, record_title=i.record_title)\n",
    "    review_objects.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titles[0], authors[0], author_viafs[0], years[0], publishers[0], publisher_viafs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector = every title word, every author name token, every publisher name token \n",
    "# ground truth will be a list of token lists, one for each book\n",
    "ground_truth_lists = []\n",
    "for e,i in enumerate(titles):\n",
    "    this_list = []\n",
    "    for z in [titles, authors, publishers]:\n",
    "        tokens = word_tokenize(z[e].lower())\n",
    "        tokens = [j.replace(\"&\", \"and\") for j in tokens]\n",
    "        tokens = [j for j in tokens if j not in string.punctuation]\n",
    "        this_list.extend(tokens)\n",
    "    ground_truth_lists.append(this_list)\n",
    "#title = titles[0], authors[0], author_viafs[0], years[0], publishers[0], publisher_viafs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'confessions', 'of', 'lord', 'byron', 'lord', 'byron', 'w.', 'a.', 'lewis', 'bettany', 'john', 'murray', 'charles', 'scribner', \"'s\", 'sons']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_lists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "all_text_merged = []\n",
    "\n",
    "# loop candidate data\n",
    "for e,t in enumerate(review_objects):\n",
    "    # true label is titles[e]\n",
    "    # get text from various attributes (title, publisher, author) \n",
    "    # tokenize\n",
    "    # remove function words and punctuation? \n",
    "    text_merged = [] \n",
    "    for z in [t.title_candidates, t.author_candidates, t.publisher_candidates]:\n",
    "        this_list = []\n",
    "        #z is a dictionary or Counter\n",
    "        for k,v in z.items():\n",
    "            #screen out 'review no title'\n",
    "            tokens = word_tokenize(k.lower())\n",
    "            tokens = [j.replace(\"&\", \"and\") for j in tokens]\n",
    "            tokens = [j for j in tokens if j not in string.punctuation]\n",
    "            #for r in range(v):\n",
    "            this_list.extend(tokens)\n",
    "        match_indices = []\n",
    "        for n,t in enumerate(this_list):\n",
    "            if t == 'review':\n",
    "                try:\n",
    "                    if this_list[n+1] == 'no':\n",
    "\n",
    "                        try:\n",
    "                            if this_list[n+2] == 'title':\n",
    "                                match_index = n\n",
    "                                match_indices.append(n)\n",
    "                        except:\n",
    "                            pass\n",
    "                except:\n",
    "                    pass\n",
    "        for m in match_indices:\n",
    "            this_list[m] = ''\n",
    "            this_list[m+1] = ''\n",
    "            this_list[m+2] = ''\n",
    "        this_list = [j for j in this_list if j !='']\n",
    "        text_merged.extend(this_list)\n",
    "    all_text_merged.append(text_merged)\n",
    "    # the true vector becomes comparison_set[-1]\n",
    "    comparison_set = [Counter(u) for u in ground_truth_lists] + [Counter(text_merged)]\n",
    "    \n",
    "    # compare \n",
    "    vectorizer = DictVectorizer()\n",
    "    X = vectorizer.fit_transform(comparison_set)\n",
    "    vectors = X.toarray()\n",
    "    # loop all, get similarity, last one is always 1.0\n",
    "    scores = []\n",
    "    for v in vectors:\n",
    "        score = cosine_similarity([v], [vectors[-1]])\n",
    "        scores.append(score)\n",
    "    all_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carnival', 'd.', 'appleton', 'and', 'company', 'd.', 'appleton', 'and', 'company', 'appleton']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_merged[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often is the right answer the top answer? how often is the correct match in the top 5?\n",
    "output = []\n",
    "for e, score_grid in enumerate(all_scores):\n",
    "    df = pd.DataFrame()\n",
    "    df['score'] = [i[0][0] for i in score_grid][:-1]\n",
    "    df['title'] = titles\n",
    "    match = []\n",
    "    for i in range(len(titles)):\n",
    "        if e == i:\n",
    "            match.append('yes')\n",
    "        else:\n",
    "            match.append('no')\n",
    "    df['match'] = match\n",
    "    output.append(df.sort_values(by=\"score\", ascending=False).reset_index(drop=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = []\n",
    "accuracy = []\n",
    "\n",
    "for r in range(1, 562):\n",
    "    results = []\n",
    "    for i in output:\n",
    "        df_n = i.iloc[:r]\n",
    "        # check if 'yes' is in df_n\n",
    "        result = len(df_n.loc[df_n['match'] == 'yes']) > 0\n",
    "        results.append(result)\n",
    "    acc = len([i for i in results if i])/len(results) \n",
    "    top_n.append(r)\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot object at 0x12ec82f28>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.lineplot(x=top_n, y=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_match = []\n",
    "top_five = []\n",
    "top_ten = []\n",
    "top_25 = []\n",
    "top_50 = []\n",
    "top_100 = []\n",
    "top_200 = []\n",
    "for i in output:\n",
    "    df_five = i.iloc[:5]\n",
    "    df_ten = i.iloc[:10]\n",
    "    df_25 = i.iloc[:25]\n",
    "    df_50 = i.iloc[:50]\n",
    "    df_100 = i.iloc[:100]\n",
    "    df_200 =i.iloc[:200]\n",
    "    top_match.append(i.iloc[0]['match'] == 'yes')\n",
    "    top_five.append(len(df_five.loc[df_five['match'] == 'yes']) > 0)\n",
    "    top_ten.append(len(df_ten.loc[df_ten['match'] == 'yes']) > 0)\n",
    "    top_25.append(len(df_25.loc[df_25['match'] == 'yes']) > 0)\n",
    "    top_50.append(len(df_50.loc[df_50['match'] == 'yes']) > 0)\n",
    "    top_100.append(len(df_100.loc[df_100['match'] == 'yes']) > 0)\n",
    "    top_200.append(len(df_200.loc[df_200['match'] == 'yes']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5597147950089126"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 55.97% whole vector\n",
    "len([i for i in top_match if i])/len(top_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308377896613191"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 73% in top five\n",
    "len([i for i in top_five if i])/len(top_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003565062388592"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% in top ten\n",
    "len([i for i in top_ten if i])/len(top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875222816399287"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#87.5% in top 25\n",
    "len([i for i in top_25 if i])/len(top_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9197860962566845"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#91% in top 50\n",
    "len([i for i in top_50 if i])/len(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536541889483066"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in top_200 if i])/len(top_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Animal Artisans; and Other Studies of Beasts and Birds'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatches = []\n",
    "mismatch_candidate_tokens = []\n",
    "for e, df in enumerate(output):\n",
    "    if df.iloc[0]['match'] != 'yes':\n",
    "        mismatches.append(df)\n",
    "        mismatch_candidate_tokens.append(all_text_merged[e])\n",
    "mismatches[2].iloc[0]['title'], mismatches[2].loc[mismatches[2]['match'] == 'yes'].iloc[0]['title'], mismatch_candidate_tokens[2]\n",
    "mismatches[2].iloc[24]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#r = requests.get(\"https://www.worldcat.org/search?q=ti%3Ahuckleberry+finn&fq=yr%3A1865..1925+%3E+%3E+x0%3Abook&qt=advanced&dblist=638\")\n",
    "# page 2 ... \"https://www.worldcat.org/search?q=ti%3Ahuckleberry+finn&fq=yr%3A1865..1925+%3E+%3E+x0%3Abook&dblist=638&start=11&qt=page_number_link\"\n",
    "bs = BeautifulSoup(r.text)\n",
    "bs.find_all(\"span\", {\"class\":\"itemPublisher\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
