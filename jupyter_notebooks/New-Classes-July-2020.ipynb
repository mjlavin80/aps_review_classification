{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from database import *\n",
    "import database.models as models\n",
    "\n",
    "# load full text from db\n",
    "aps_details_single = models.Review().query.filter(models.Review.status.in_(('needs_crosscheck', 'done'))).filter(models.Review.review_type == 'single_focus').all()\n",
    "aps_details_w_title = [i for i in aps_details_single if i.reviewed_book_title != '' and i.reviewed_book_title is not None]\n",
    "len(aps_details_w_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [i.reviewed_book_title for i in aps_details_single if i.reviewed_book_title != '' and i.reviewed_book_title is not None]\n",
    "all_text = [i.full_text for i in aps_details_single if i.reviewed_book_title != '' and i.reviewed_book_title is not None]\n",
    "\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = [i.reviewed_book_publisher for i in aps_details_single if i.reviewed_book_publisher !='']\n",
    "known_publishers = list(set(publishers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzysearch import find_near_matches\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from random import shuffle\n",
    "\n",
    "def is_word(word):\n",
    "    \"\"\"\n",
    "    Returns true if word is found in sym_spell dictionary, otherwise returns false.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fake = sym_spell._words[word]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_surname(surname):\n",
    "    \"\"\"\n",
    "    Returns true if name is found in sym_spell author surname dictionary, otherwise returns false.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fake = author_surname_dict._words[surname.lower()]\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fix_hyphenated_words(toks):\n",
    "    \"\"\"\n",
    "    Replaces hyphenated words with single word.\n",
    "    \"\"\"\n",
    "    dash_indices = find_dashes(toks)\n",
    "    to_be_deleted = []\n",
    "    for i in dash_indices:\n",
    "        #if neither are words, e.g. pieces of names or misspellings\n",
    "        if (is_word(toks[i][:-1])==False or is_word(toks[i+1])==False):\n",
    "            #replace first item with combined, delete second item\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "            #if combined is a word\n",
    "        elif (is_word((toks[i][:-1] + toks[i+1]))):\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "        elif (is_surname((toks[i][:-1] + toks[i+1]))):\n",
    "            to_be_deleted.append(i+1)\n",
    "            toks[i] = (toks[i][:-1] + toks[i+1])\n",
    "        else:\n",
    "            pass\n",
    "    #do this after so u don't heck up the indices\n",
    "    toks = [w for i, w in enumerate(toks) if i not in to_be_deleted]\n",
    "    return toks\n",
    "\n",
    "def find_dashes(toks):\n",
    "    \"\"\"\n",
    "    Returns list of indices for words ending in dashes.\n",
    "    \"\"\"\n",
    "    dash_indices = [i for i, word in enumerate(toks) if (len(word)>1) and (word.endswith('-'))]\n",
    "    return dash_indices\n",
    "\n",
    "def remove_function_head(sequence):\n",
    "    if sequence:\n",
    "        if sequence[0][0].islower():\n",
    "            sequence.pop(0)\n",
    "            return remove_function_head(sequence)\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def remove_function_tail(sequence):\n",
    "    if sequence: \n",
    "        if sequence[-1].lower() in stopwords.words('english'):\n",
    "            sequence.pop()\n",
    "            return remove_function_tail(sequence)\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_honorifics(sequence):\n",
    "    honorifics = \"\"\"Doctor,Dr,Mr,Mrs,Miss,Msgr,Monsignor,Rev,Reverend,Hon,Honorable,Honourable,Prof,Professor,Madame,Madam,Lady,Lord,Sir,Dame,Master,Mistress,Princess,Prince,Duke,Duchess,Baron,Father,Chancellor,Principal,President,Pres,Warden,Dean,Regent,Rector,Provost,Director\"\"\"\n",
    "    honorific_list = honorifics.lower().split(',')\n",
    "    if sequence:\n",
    "        if len(sequence) == 1 and sequence[0].lower() in honorific_list:\n",
    "            return None\n",
    "        else:\n",
    "            return sequence\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def cull_title(text_block, patterns):\n",
    "    \n",
    "    for i in patterns:\n",
    "        text_block = text_block.replace(i, \"\")\n",
    "        \n",
    "    culled_title_candidates = [list(),]\n",
    "    for token in word_tokenize(text_block):\n",
    "        if token[0].isupper() or token.lower() in stopwords.words('english') or token in string.punctuation:\n",
    "            if len(culled_title_candidates[-1]) > 0:\n",
    "                if token not in string.punctuation:\n",
    "                    culled_title_candidates[-1].append(token)\n",
    "            else:\n",
    "                if token[0].isupper():\n",
    "                    culled_title_candidates[-1].append(token)\n",
    "        else:\n",
    "            if len(culled_title_candidates[-1]) > 0:\n",
    "                culled_title_candidates.append(list())\n",
    "    # remove any culled_title_candidate if it's just an honorific\n",
    "    candidates_no_tail = []\n",
    "    for sequence in culled_title_candidates:\n",
    "        \n",
    "        if len(sequence) > 0:\n",
    "            #remove lowercase function word heads and tails recursively\n",
    "            sequence = remove_function_tail(sequence)\n",
    "            sequence = remove_function_head(sequence)\n",
    "            sequence = remove_honorifics(sequence)\n",
    "            if sequence:\n",
    "                candidates_no_tail.append(\" \".join(sequence).lower())\n",
    "    non_titles = ['new publications','the latest books and authors', 'latest books and authors', 'books and authors', \\\n",
    "                  'the latest books', 'latest books', 'minor notices', 'no title', 'a book-shelf for the month', 'book-shelf', \\\n",
    "                  'some recent books', 'some recent fiction', 'recent books', 'recent fiction', 'latest fiction', 'current books', 'current fiction' \\\n",
    "                   'books and authors', 'our book table', 'current literature', 'literature', 'book reviews' 'reviews', 'review',]\n",
    "    candidates_tidy = []\n",
    "    for i in candidates_no_tail:\n",
    "        title = i\n",
    "        for z in non_titles:\n",
    "            title = title.replace(z, '')\n",
    "        title = title.strip()\n",
    "        if title != '':\n",
    "            candidates_tidy.append(title)\n",
    "    return candidates_tidy\n",
    "\n",
    "class ReviewObject():\n",
    "    \"\"\"\n",
    "    object class for a book review, including variables for labels, full text, parsed entities, match data, etc.\n",
    "    tokens_raw (list)\n",
    "    tokens_tidy (list)\n",
    "    title_candidates {candidate_string: score, candidate_string: score}\n",
    "    author_candidates {candidate_string: score, candidate_string: score}\n",
    "    publisher_candidates {candidate_string: score, candidate_string: score}\n",
    "    candidate_mappings {\"authors\": { candidate_string: {match_string: match_score, match_string: match_score}}, \"titles\": None, \"publishers\": None}\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, title, full_text, **kwargs):\n",
    "        # instantiate with metdata from db\n",
    "        self.title = title\n",
    "        self.full_text = full_text\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.make_tokens()\n",
    "        self.make_tidy()\n",
    "        self.extract_title_candidates()\n",
    "        self.extract_author_candidates()\n",
    "        self.extract_publisher_candidates()\n",
    "        self.map_candidates_to_entities()\n",
    "        self.select_top_matches()\n",
    "        \n",
    "    def make_tokens(self):\n",
    "        self.tokens_raw = word_tokenize(self.full_text)\n",
    "\n",
    "    def make_tidy(self):\n",
    "        #adding space around certain problem punctuation\n",
    "        txt = re.sub(',',' , ',self.full_text)\n",
    "        txt = re.sub(';',' ; ',txt)\n",
    "        txt = re.sub(':',' : ',txt)\n",
    "        txt = re.sub('\"',' \" ',txt)\n",
    "        txt = re.sub('&',' & ',txt)\n",
    "        txt = re.sub(\"'(?!s)\",\" ' \" ,txt)\n",
    "        # remove extra whitespace\n",
    "        txt = re.sub(' +',' ',txt)\n",
    "        # fix hyphenated words\n",
    "        txt = ' '.join(fix_hyphenated_words(txt.split()))\n",
    "        #putting space back\n",
    "        txt = re.sub(' , ',', ',txt)\n",
    "        txt = re.sub(' ; ','; ',txt)\n",
    "        txt = re.sub(' : ',': ',txt)\n",
    "        #and fixing hyphen issues\n",
    "        txt = re.sub('-(?!\\w)',' - ',txt)\n",
    "        self.text_tidy = re.sub('(?<!\\w)-',' - ',txt)\n",
    "        self.tokens_tidy = word_tokenize(self.text_tidy)\n",
    "        \n",
    "        self.text_tidy_lower = self.text_tidy.lower()\n",
    "        self.tokens_tidy_lower = word_tokenize(self.text_tidy_lower)\n",
    "        \n",
    "    \n",
    "    def extract_title_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Look for capitalized string in review title\n",
    "        2. Look for before and after cues, get capitalized strings\n",
    "        3. If no cues, look for capitalized strings\n",
    "        4. Cull obvious false positives \n",
    "        \"\"\"\n",
    "        patterns = [\"chapter of\", \"chapters of\", \"latest\", \"book called\", \"volume called\", \"novel called\", \"volume of\", \"edition of\", \"novel\", \\\n",
    "                    \"study of\", \"entitled\", \"the author of\", \"with the title\", \"the manner in which\" \\\n",
    "                    \"book\", \"story\", \"life of\"]\n",
    "        \n",
    "        title_candidates = []\n",
    "        if self.record_title:\n",
    "            title_candidates.extend(re.findall(\"\\\".+\\\"\", self.record_title))\n",
    "            title_candidates.extend(re.findall(\"\\'.+\\'\", self.record_title))\n",
    "            \n",
    "        culled_title_candidates_all = []\n",
    "        for text_block in title_candidates:\n",
    "            culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "                \n",
    "        if len(culled_title_candidates_all) == 0:\n",
    "            title_candidates.append(self.record_title)\n",
    "            culled_title_candidates_all = []\n",
    "            for text_block in title_candidates:\n",
    "                culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "\n",
    "            for i in patterns:\n",
    "                pattern = \"\".join([i, \".+?\\.\"])\n",
    "                title_candidates.extend(re.findall(pattern, self.text_tidy))\n",
    "\n",
    "\n",
    "            for text_block in title_candidates:\n",
    "                culled_title_candidates_all.extend(cull_title(text_block, patterns))\n",
    "        \n",
    "        self.title_candidates = Counter(culled_title_candidates_all)\n",
    "    \n",
    "    def extract_author_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Look for honorifics, try to extrapolate surnames\n",
    "        2. Look for surnames in review title\n",
    "        2. Look for before and after cues + surnames in text\n",
    "        3. If no cues, look for capitalized N-grams ending with surnames\n",
    "        4. Score each candidate entry based on how it was found\n",
    "        \"\"\"\n",
    "        titles = \"\"\"Doctor,Dr,Mr,Mrs,Miss,Msgr,Monsignor,Rev,Reverend,Hon,Honorable,Honourable,Prof,Professor,Madame,Madam,Lady,Lord,Sir,Dame,Master,Mistress,Princess,Prince,Duke,Duchess,Baron,Father,Chancellor,Principal,President,Pres,Warden,Dean,Regent,Rector,Provost,Director\"\"\"\n",
    "        titles = titles.split(',')\n",
    "\n",
    "        full_names = {}\n",
    "\n",
    "        for e,i in enumerate(self.tokens_tidy):\n",
    "            maybe_title = \"\".join([z for z in i if z.isalpha()])\n",
    "            if maybe_title in titles:\n",
    "\n",
    "                surname = []\n",
    "                for p in [e+1, e+2, e+3]:\n",
    "                    try:\n",
    "                        if self.tokens_tidy[p][0].isupper():\n",
    "                            surname.append(self.tokens_tidy[p])\n",
    "                    except:\n",
    "                        pass\n",
    "                if len(surname) > 0:\n",
    "                    surname = \" \".join(surname).replace(\"'s\", \"\")\n",
    "                    surname_cleaned = []\n",
    "                    for s in surname:\n",
    "                        if s not in '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~':\n",
    "                            surname_cleaned.append(s)\n",
    "                    surname_cleaned = \"\".join(surname_cleaned)\n",
    "                    try:\n",
    "                        check = full_names[surname]\n",
    "                    except:\n",
    "                        full_names[surname] = {}\n",
    "                    try:\n",
    "                        full_names[surname]['title'].append(maybe_title)\n",
    "                    except:\n",
    "                        full_names[surname]['title'] = [maybe_title,]\n",
    "                    try:\n",
    "                        full_names[surname]['surname_cleaned'].append(surname_cleaned)\n",
    "                    except:\n",
    "                        full_names[surname]['surname_cleaned'] = [surname_cleaned,]\n",
    "        \n",
    "        for surname in full_names.keys():\n",
    "            s = surname.split()\n",
    "            for e, i in enumerate(self.tokens_tidy):\n",
    "                if self.tokens_tidy[e:e+len(s)] == s:\n",
    "                    forename = \"\".join([x for x in self.tokens_tidy[e-1] if x.isalpha()])\n",
    "                    if forename.istitle() and forename not in titles:\n",
    "                        try:\n",
    "                            full_names[surname]['forename'].append(forename)\n",
    "                        except:\n",
    "                            full_names[surname]['forename'] = [forename,]\n",
    "                try:\n",
    "                    forenames = full_names[surname]['forename']\n",
    "                except:\n",
    "                    full_names[surname]['forename'] = []\n",
    "\n",
    "        for name in full_names.keys():\n",
    "            for i in full_names[name]['forename']:\n",
    "                try: \n",
    "                    full_names[name]['full_name'].append(i + \" \" + name)\n",
    "                except:\n",
    "                    full_names[name]['full_name'] = [i + \" \" + name,]\n",
    "            try:\n",
    "                full = full_names[surname]['full_name']\n",
    "            except:\n",
    "                full_names[name]['full_name'] = []\n",
    "        \n",
    "        full_name_candidates = {}\n",
    "\n",
    "        for n in full_names.keys():\n",
    "            for f in full_names[n]['full_name']:\n",
    "                try:\n",
    "                    full_name_candidates[f] += 1\n",
    "                except:\n",
    "                    full_name_candidates[f] = 1\n",
    "        \n",
    "        author_surname_candidates = {}\n",
    "        \n",
    "        # add title and surnames\n",
    "        for n,o in full_names.items():\n",
    "            for i in o['surname_cleaned']:\n",
    "                try:\n",
    "                    author_surname_candidates[i] +=1\n",
    "                except:\n",
    "                    author_surname_candidates[i] =1\n",
    "                \n",
    "                # check if surname in a full name\n",
    "                name_part = False\n",
    "                for full in full_name_candidates.keys():\n",
    "                    if i in full:\n",
    "                        name_part = True\n",
    "                if not name_part:\n",
    "                    try: \n",
    "                        full_name_candidates[i] += 1\n",
    "                    except:\n",
    "                        full_name_candidates[i] = 1\n",
    "                        \n",
    "        self.author_candidates = full_name_candidates\n",
    "        self.author_surname_candidates = author_surname_candidates\n",
    "    \n",
    "    def extract_publisher_candidates(self):\n",
    "        \"\"\"\n",
    "        1. Fuzzy match against known publishers, and count mentions\n",
    "        2. If no matches, look for pub ends and capitalization, and count each candidate\n",
    "        \"\"\"\n",
    "        pub_ends = ['company','co','incorporated','inc','firm','press','group','publishers','publishing', \\\n",
    "            'publications','pub','books','ltd','limited','society','house','associates', 'book', 'university']\n",
    "        \n",
    "        self.publisher_candidates = {}\n",
    "        \n",
    "        #this list is defined outside the class\n",
    "        for p in known_publishers:\n",
    "            #base fuzziness on length of pubname\n",
    "            if len(p) < 6:\n",
    "                fuzz=0\n",
    "            elif len(p) > 5 and len(p) < 10:\n",
    "                fuzz=1\n",
    "            elif len(p) > 9 and len(p) < 15:\n",
    "                fuzz=2\n",
    "            else:\n",
    "                fuzz=3\n",
    "            \n",
    "            matches = find_near_matches(p, self.text_tidy, max_l_dist=fuzz)\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                match_strings = [self.text_tidy[m.start:m.end] for m in matches]\n",
    "                for i in match_strings:\n",
    "                    try:\n",
    "                        self.publisher_candidates[p] += 1\n",
    "                    except:\n",
    "                        self.publisher_candidates[p] = 1\n",
    "                        \n",
    "        if self.publisher_candidates == {}:\n",
    "            publisher_candidates = [list(),]\n",
    "            for token in self.tokens_tidy:\n",
    "                if token[0].isupper() or token in ['and', '&'] or token in string.punctuation:\n",
    "                    if len(publisher_candidates[-1]) > 0:\n",
    "                        if token not in string.punctuation:\n",
    "                            publisher_candidates[-1].append(token) \n",
    "                    else:\n",
    "                        if token[0].isupper():\n",
    "                            publisher_candidates[-1].append(token)\n",
    "                else:\n",
    "                    if len(publisher_candidates[-1]) > 0:\n",
    "                        publisher_candidates.append(list())\n",
    "    \n",
    "            matches = []\n",
    "            for sequence in publisher_candidates:\n",
    "                for token in sequence:\n",
    "                    normed_token = token.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "                    if normed_token in pub_ends:\n",
    "                        matches.append(\" \".join(sequence))\n",
    "                        break\n",
    "                        \n",
    "            self.publisher_candidates = Counter(matches)\n",
    "\n",
    "    def map_candidates_to_entities(self):\n",
    "        \"\"\"\n",
    "        1. Set keys with no values if no candidates \n",
    "        2. Map and score publisher matches\n",
    "        3. Map and score author matches\n",
    "        4. Map and score title matches\n",
    "        5. Map and score whole book matches\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def percentiles(self, mycounter):\n",
    "        if len(mycounter.most_common()) > 0:\n",
    "            a = list([i[0] for z in range(i[1])] for i in mycounter.most_common())\n",
    "            flat_list = [item for sublist in a for item in sublist]\n",
    "            results = {}\n",
    "            for i in range(1000):\n",
    "                shuffle(flat_list)\n",
    "                try:\n",
    "                    results[flat_list[0]] +=1\n",
    "                except:\n",
    "                    results[flat_list[0]] =1\n",
    "            return Counter({i[0]:i[1]/1000 for i in results.items()})\n",
    "        else:\n",
    "            return Counter()\n",
    "        \n",
    "    def select_top_matches(self):\n",
    "        \"\"\"\n",
    "        Return top publishers, titles, and authors thought to be the correct matches \n",
    "        \"\"\"\n",
    "        self.title_candidates = Counter(self.title_candidates)\n",
    "        self.author_candidates = Counter(self.author_candidates)\n",
    "        self.author_surname_candidates = Counter(self.author_surname_candidates)\n",
    "        self.publisher_candidates = Counter(self.publisher_candidates)\n",
    "        self.top_titles = self.percentiles(self.title_candidates)\n",
    "        self.top_authors = self.percentiles(self.author_candidates)\n",
    "        self.top_author_surnames = self.percentiles(self.author_surname_candidates)\n",
    "        self.top_publishers = self.percentiles(self.publisher_candidates)\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Review Object titled '%s' with the following instance variables: %s \" % (self.title, \"'\"+\"', '\".join(self.__dict__.keys())+\"'\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Confessions of Lord Byron',\n",
       " [('lord byron self-revealed', 0.331), ('mir', 0.184), ('cambridge', 0.168)],\n",
       " [('W. A. Lewis', 0.231), ('Johnson', 0.201), ('Byron', 0.2)],\n",
       " [('Clare', 0.216), ('Byron', 0.206), ('Johnson', 0.198)],\n",
       " [(\"Charles Scribner's Sons\", 0.502), (\"Scribner's\", 0.498)])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ReviewObject(title=titles[1], full_text=all_text[1], record_title=aps_details_w_title[1].record_title)\n",
    "#test.title, test.author_candidates, test.publisher_candidates\n",
    "test.title, test.top_titles.most_common(3), test.top_authors.most_common(3), test.top_author_surnames.most_common(3), test.top_publishers.most_common(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorObject():\n",
    "    \"\"\"\n",
    "    object class for a known author (not a match candidate)\n",
    "    initialize with viaf uri or string\n",
    "    try to get data from local db\n",
    "    if no data, get from API and store to local db \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class PublisherObject():\n",
    "    \"\"\"\n",
    "    object class for a known publisher (not a match candidate)\n",
    "    initialize with viaf uri or string\n",
    "    try to get data from local db\n",
    "    if no data, get from API and store to local db \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BookObject():\n",
    "    \"\"\"\n",
    "    object class for a known book, including references to associated authors and publishers\n",
    "    initialize with a title string\n",
    "    try to get data from local db\n",
    "    if no data, get from worldcat, wikidata, hathitrust, lccn, viaf ??? and store to local db \n",
    "     \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
